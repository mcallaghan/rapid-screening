diff --git a/manuscript/main.tex b/manuscript/main.tex
index b0474ba..0fb255b 100644
--- a/manuscript/main.tex
+++ b/manuscript/main.tex
@@ -71,10 +71,12 @@
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 \usepackage{todonotes}
+\usepackage{bm}
 \usepackage{graphicx}
 \usepackage{amsmath}
 \usepackage{url}
 \usepackage{subcaption}
+\usepackage{caption}
 \usepackage{booktabs}
 %\def\includegraphic{}
 %\def\includegraphics{}
@@ -225,33 +227,33 @@
 		%\end{fmbox}% uncomment this for twcolumn layout
 		
 	\end{frontmatter}
-
-%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
-%%                                          %%
-%% The Main Body begins here                %%
-%%                                          %%
-%% Please refer to the instructions for     %%
-%% authors on:                              %%
-%% http://www.biomedcentral.com/info/authors%%
-%% and include the section headings         %%
-%% accordingly for your article type.       %%
-%%                                          %%
-%% See the Results and Discussion section   %%
-%% for details on how to create sub-sections%%
-%%                                          %%
-%% use \cite{...} to cite references        %%
-%%  \cite{koon} and                         %%
-%%  \cite{oreg,khar,zvai,xjon,schn,pond}    %%
-%%  \nocite{smith,marg,hunn,advi,koha,mouse}%%
-%%                                          %%
-%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
-
-%%%%%%%%%%%%%%%%%%%%%%%%% start of article main body
-% <put your article body there>
-
-%%%%%%%%%%%%%%%%
-%% Background %%
-%%
+	
+	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
+	%%                                          %%
+	%% The Main Body begins here                %%
+	%%                                          %%
+	%% Please refer to the instructions for     %%
+	%% authors on:                              %%
+	%% http://www.biomedcentral.com/info/authors%%
+	%% and include the section headings         %%
+	%% accordingly for your article type.       %%
+	%%                                          %%
+	%% See the Results and Discussion section   %%
+	%% for details on how to create sub-sections%%
+	%%                                          %%
+	%% use \cite{...} to cite references        %%
+	%%  \cite{koon} and                         %%
+	%%  \cite{oreg,khar,zvai,xjon,schn,pond}    %%
+	%%  \nocite{smith,marg,hunn,advi,koha,mouse}%%
+	%%                                          %%
+	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
+	
+	%%%%%%%%%%%%%%%%%%%%%%%%% start of article main body
+	% <put your article body there>
+	
+	%%%%%%%%%%%%%%%%
+	%% Background %%
+	%%
 	
 	%%%%%%%%%%%%%%%%%%%%%%%%
 	%% Introduction
@@ -261,11 +263,11 @@
 	Interventions have been proposed at various points in order to reduce the human effort required to produce systematic reviews and other forms of evidence synthesis.
 	A major strand of the literature works on screening: the identification of relevant documents in a set of documents whose relevance is uncertain \cite{OMara-Eves2015}. 
 	This is a time consuming and repetitive task, and in a research environment with constrained resources and increasing amounts of literature, this may limit the scope of the evidence synthesis projects undertaken.
-	Several papers have developed Active Learning (AL) approaches \cite{miwa2014, Wallace2010a, Wallace2010, Jonnalagadda2013, Przybya2018} to reduce the time required to screen documents. This paper sets out how current approaches are  unsuitable in practice, and outlines and evaluates a small modification that would make AL systems ready for live reviews.
+	Several papers have developed Active Learning (AL) approaches \cite{miwa2014, Wallace2010a, Wallace2010, Jonnalagadda2013, Przybya2018} to reduce the time required to screen documents. This paper sets out how current approaches are  unreliable in practice, and outlines and evaluates modifications that would make AL systems ready for live reviews.
 	
 	Active learning is an iterative process where documents screened by humans are used to train a machine learning model to predict the relevance of unseen papers \cite{Settles2009}.
 	The algorithm chooses which studies will next be screened by humans, often those which are likely to be relevant or about which the model is uncertain, in order to generate more labels to feed back to the machine. 
-	By prioritising those studies most likely to be relevant, a human reviewer most often identifies all relevant studies - or a given proportion of relevant studies (recall) - before having seen all the documents in the corpus. 
+	By prioritising those studies most likely to be relevant, a human reviewer most often identifies all relevant studies -- or a given proportion of relevant studies (described by recall: the number of relevant studies identified divided by the total number of relevant studies) -- before having seen all the documents in the corpus. 
 	The proportion of documents not yet seen by the human when they reach the given recall threshold is referred to as the work saved. This represents the proportion of documents that they do not have to screen, which they would have had to without machine learning.
 	
 	Machine learning applications are often evaluated using sets of documents from already completed systematic reviews for which inclusion or exclusion labels already exist. 
@@ -273,28 +275,30 @@
 	In live review settings, however, recall remains unknown until all documents have been screened. 
 	In order for work to really be saved, reviewers have to stop screening while uncertain about recall. 
 	This is particularly problematic in systematic reviews because low recall increases the risk of bias \cite{Lefebvre2011}.
-	The lack of appropriate stopping criteria has therefore been identified as a research gap \cite{bannach-brown2019, Marshall2019}, although some approaches have been suggested. These fall into the following categories:
+	The lack of appropriate stopping criteria has therefore been identified as a research gap \cite{bannach-brown2019, Marshall2019}, although some approaches have been suggested. These have most commonly fallen into the following categories:
 	\begin{itemize}
 		\item \textbf{Sampling criteria:} Reviewers estimate the number of relevant documents by taking a random sample at the start of the process. They stop when this number, or a given proportion of it, has been reached \cite{Shemilt2014}
 		\item \textbf{Heuristics:} Reviewers stop when a given number of irrelevant articles are seen in a row \cite{Jonnalagadda2013, Przybya2018}. 
 		\item \textbf{Pragmatic criteria:} Reviewers stop when they run out of time \cite{miwa2014}. 
+		\item \textbf{Novel automatic stopping criteria:} Recent papers have proposed more complicated novel systems for automatically deciding when to stop screening \cite{Yu2019, DiNunzio2018, Howard2020}
 	\end{itemize}
 	
-	We review these methods in the following section and discuss their theoretical limitations. They are then tested on several previous systematic review datasets.
-	We demonstrate theoretically and with our experimental results, that these methods can not deliver consistent levels of work savings or recall - particularly across different domains, or datasets with different properties \cite{OMara-Eves2015}.  
+	
+	We review the first three classes of these methods in the following section and discuss their theoretical limitations. They are then tested on several previous systematic review datasets.
+	We demonstrate theoretically and with our experimental results, that these three classes of methods can not deliver consistent levels of work savings or recall - particularly across different domains, or datasets with different properties \cite{OMara-Eves2015}. We also discuss the limitations of novel automatic stopping criteria, which have all demonstrated promising results, but do not achieve a given level of recall in a reliable or reportable way.   
 	Without the reliable or reportable achievement of a desired level of recall, deployment of AL systems in live reviews remains challenging.
 	
 	This study proposes a system for estimating the recall based on random sampling of remaining documents. 
 	We use a simple statistical method to iteratively test a null hypothesis that the recall achieved is less than a given target recall. If the hypothesis can be rejected, we conclude that the recall target has been achieved with a given confidence level and screening can be stopped.
-	This allows AL users to predefine a target in terms of uncertainty and recall, so that they can make transparent, easily communicable statements like ``A recall of more than 95\% was achieved with a confidence of 95\%''.
+	This allows AL users to predefine a target in terms of uncertainty and recall, so that they can make transparent, easily communicable statements like ``A recall of more than 95\% was achieved with a confidence of more than 95\%''.
+	
 	
-	The information retrieval literature discusses similar stopping criteria for ranking algorithms like BM25 and variants \cite{DiNunzio2018, Yu2019}. However, the estimators they use to determine the recall rely on the specific ranking functions and depend on their search input. Therefore, the quality of the estimation depends on the adequacy of the model. Our approach, on the contrary, is independent of model choice or model performance. 
 	
-	In the remainder of the paper, we first discuss in detail the shortcomings of existing stopping criteria. Then, we introduce our new criterion based on a hypergeometric test. We evaluate our stopping criteria, and compare their performance with heuristic and sampling based criteria on real-world systematic review datasets on which AL systems have previously been tested \cite{Cohen2006, Yu2019, Terasawa2009, Castaldi2009}.
+	In the remainder of the paper, we first discuss in detail the shortcomings of existing stopping criteria. Then, we introduce our new criteria based on a hypergeometric test. We evaluate our stopping criteria, and compare their performance with heuristic and sampling based criteria on real-world systematic review datasets on which AL systems have previously been tested \cite{Cohen2006, Yu2019, Terasawa2009, Castaldi2009}.
 	
 	\section*{Methods Review}
-
-
+	
+	
 	We start by explaining the sampling and heuristic based stopping criteria and discussing their methodological limitations. 
 	
 	\subsection*{Sampling Based Stopping Criteria}
@@ -304,127 +308,184 @@
 	Reviewers continue to screen until this number, or a proportion of it corresponding to the desired level of recall, is reached.
 	
 	
-	However, the estimation of the BIR fails to correctly take into account sampling uncertainty \footnote{Although Shemilt et al. \cite{Shemilt2014} employ a method  to choose a sample size based on uncertainty, they fail to acknowledge the potential implications for recall of their choice. Their margin of error of 0.0025 and observed proportion of relevant studies of 0.0005 translate to estimates of $400 \pm 451$ relevant results. To reduce the margin of error to $\pm 5\%$ of estimated relevant studies, they would have had to screen 638,323 out of 804,919 results. See the notebook \url{https://github.com/mcallaghan/rapid-screening/blob/master/analysis/bir_theory.ipynb} that accompanies this paper for a detailed discussion}. 
-	This uncertainty is crucial, as errors can have severe consequences. If the estimated number of relevant documents is even one unit above the true value, then no work savings will be achieved. If the number of relevant documents is underestimated, then the recall achieved will be less than 100\%.
+	However, the estimation of the BIR fails to correctly take into account sampling uncertainty \footnote{Although Shemilt et al. \cite{Shemilt2014} employ a method  to choose a sample size based on uncertainty, they fail to acknowledge the potential implications for recall of their choice. Their margin of error of 0.0025 and observed proportion of relevant studies of 0.0005 translate to estimates of $400 \pm 451$ relevant results. To reduce the margin of error to $\pm 5\%$ of estimated relevant studies, they would have had to screen 638,323 out of 804,919 results. See the notebook \url{https://github.com/mcallaghan/rapid-screening/blob/master/analysis/bir_theory.ipynb} that accompanies this paper for a detailed discussion.}. 
+	This uncertainty is crucial, as errors can have severe consequences. Let us assume that users will stop screening when they have identified 95\% of the relevant number of documents. If the estimated number of relevant documents is more than the true number of relevant documents divided by 0.95, then the users will never see 95\% of the estimated number.  This means that they will keep screening until they have seen all documents, and no work savings will be achieved. Conversely, if the number of relevant documents is underestimated by even a single unit, then the recall achieved will be lower than the target.
 	
 	The number of relevant documents drawn without replacement from a finite sample of documents follows the hypergeometric distribution. 
-	Figure \ref{fig:bir_error} shows the distribution of the predicted number of documents after drawing 1,000 documents from a total of 20,000 documents, where 500 documents (2.5\%) are relevant. The left shaded portion of the graph shows all the cases where the recall will be less than 95\%. This occurs 35.91\% of the time. The right shaded portion of the graph shows the cases where the number of relevant documents is overestimated and no work savings could be made. This occurs 46.24\% of the time. In only 17.85\% of cases can work savings be achieved while still achieving a recall of at least 95\%. 
+	Figure \ref{fig:bir_error} shows the distribution of the predicted number of documents after drawing 1,000 documents from a total of 20,000 documents, where 500 documents (2.5\%) are relevant. The left shaded portion of the graph shows all the cases where the recall will be less than 95\%. This occurs 48\% of the time. The right shaded portion of the graph shows the cases where the number of relevant documents is overestimated so much that no work savings could be made to achieve a target recall of 95\%. This occurs 29\% of the time. In only 23\% of cases can work savings be achieved while still achieving a recall of at least 95\%. 
+	
+	Figure \ref{fig:bir_error_distribution} shows the probability distribution of these errors according to the sample size. Even in very large samples both types of error remain frequent.
+	This shows how baseline estimation inevitably offers poor reliability, either in terms of recall or in work saved.
 	
-	Figure \ref{fig:bir_error_distribution} shows the probability distribution of these errors according to the sample size. Even in very large samples both types of error remain frequent).
-	This shows how baseline estimation inevitably offers poor reliability, both in terms of recall and in work saved.
-
-
 	\begin{figure*}
-	\centering
-	\begin{subfigure}[b]{0.475\textwidth}   
-		\centering 
-		\includegraphics[width=\textwidth]{../images/bir_errors}
-		\caption[]%
-		{{\small Error types after 1,000 documents \\}}    
-		\label{fig:bir_error}
-	\end{subfigure}
-	\hfill
-	\begin{subfigure}[b]{0.475\textwidth}   
-		\centering 
-		\includegraphics[width=\textwidth]{../images/bir_error_distribution}
-		\caption[]%
-		{{\footnotesize Error types across sample sizes}}    
-		\label{fig:bir_error_distribution}
-	\end{subfigure}
-
-	\caption{\small Distribution of under- or over-estimation errors using the BIR sampling method in a dataset of 20,000 documents of which 500 are relevant. Panel (a) shows the probability distribution of the estimated number of relevant documents after a sample of 1,000 documents. Panel (b) shows the probability of each type of error according to the sample size.} 
-	\label{bir-sampling}
-\end{figure*}
-
-\begin{figure}
-	\includegraphics[width=\linewidth]{../images/proportions_1.png}
-	\includegraphics[width=\linewidth]{../images/proportions_2.png}
-	\caption{Similar low proportions of relevant documents in unseen documents with different consequences for recall. The top bar shows a random distribution of relevant documents (green) and irrelevant documents (red) at a given proportion of relevance. The bottom bar  shows distributions of relevant and irrelevant documents in hypothetical sets of seen (right) and unseen (left - transparent) documents.}
-	\label{unseen-proportions}
-\end{figure}
-
-
+		\centering
+		\begin{subfigure}[b]{0.475\textwidth}   
+			\centering 
+			\includegraphics[width=\textwidth]{2_figs_bir_errors.pdf}
+			\caption[]%
+			{{\small Error types after 1,000 documents \\}}    
+			\label{fig:bir_error}
+		\end{subfigure}
+		\hfill
+		\begin{subfigure}[b]{0.475\textwidth}   
+			\centering 
+			\includegraphics[width=\textwidth]{2_figs_bir_error_distribution.pdf}
+			\caption[]%
+			{{\footnotesize Error types across sample sizes}}    
+			\label{fig:bir_error_distribution}
+		\end{subfigure}
 		
-\subsubsection*{Heuristic Stopping Criteria}
-
-Some studies give the example of heuristic stopping criteria based on drawing a given number of irrelevant articles in a row \cite{Jonnalagadda2013, Przybya2018}. 
-We take this as a proxy for estimating that the proportion of documents remaining in the unseen documents is low. 
-We find this a promising intuition, but argue that 1) it ignores uncertainty, as discussed in relation to the previous method; and 2) it misunderstands the significance of a low proportion of relevant documents in estimating the recall.
-
-Figure \ref{unseen-proportions} illustrates this second point. 
-We show two scenarios with identical low proportions of relevant documents observed in the unseen documents.
-In the top figure, machine learning (ML) has performed well, and 74\% of the screened documents were relevant. 
-In the bottom figure,  ML has performed less well, and only 26\% of the screened documents were relevant.
-In both cases, only 2\% of unseen documents are relevant, but 2\% of a larger number means more relevant documents are missed.
-Recall is not simply a function of the relevance of unseen documents, but also of the number of unseen documents. 
-This also means that where ML has performed well (as in the top figure), a low proportion of relevant documents in those not yet checked is indicative of lower recall than where ML has performed less well.
-Likewise, where the proportion of relevant documents in the whole corpus is low, a similarly low proportion of relevant documents is likely to be observed, even when true recall is low.
-
-
-
-\subsubsection*{Other stopping criteria}
-
-Wallace et al. \cite{Wallace2010a} develop a ``simple, operational stopping criterion'': stopping after half the documents have been screened. Although the criterion worked in their experiment, it is unclear how this could be generalised, and its development depended on knowledge of the true relevance values. 
-Jonnalagadda and Petitti \cite{Jonnalagadda2013} note that ``the reviewer can elect to end the process of classifying documents at any point, recognizing that stopping before reviewing all documents involves a trade-off of lower recall for reduced workload'', although clearly the reviewer lacks information about probable recall.
-Yu and Menzies \cite{Yu2019} adopt a more complicated stopping criterion which allows the user to target a specific level of recall. However, reviewers are not given the opportunity to specify a confidence level, and for two of the four datasets in which they tested their criteria, the median achieved recall at a stopping criteria targeting 95\% recall was below 95\%. Di Nunzio \cite{DiNunzio2018} also present an innovative stopping criteria, but it does not take into account uncertainty, and produces results \textit{near} a target recall threshold, rather than above it in a reliable proportion of cases. 
-These last examples are promising developments, but fail to take into account the needs of live systematic reviews, where the reliability of and ease of communication about recall are paramount.
-
-\subsection*{Methods}
-
-\subsection*{A Statistical Stopping Criterion for Active Learning}
-
+		\caption{\small Distribution of under- or over-estimation errors using the BIR sampling method in a dataset of 20,000 documents of which 500 are relevant. Panel (a) shows the probability distribution of the estimated number of relevant documents after a sample of 1,000 documents. Panel (b) shows the probability of each type of error according to the sample size.} 
+		\label{bir-sampling}
+	\end{figure*}
+	
+	\begin{figure}
+		\includegraphics[width=\linewidth]{2_figs_proportions_1.png}
+		\includegraphics[width=\linewidth]{2_figs_proportions_2.png}
+		\caption{Similar low proportions of relevant documents in unseen documents with different consequences for recall. The top bar shows a random distribution of relevant documents (green) and irrelevant documents (red) at a given proportion of relevance. The bottom bar  shows distributions of relevant and irrelevant documents in hypothetical sets of seen (right) and unseen (left - transparent) documents.}
+		\label{unseen-proportions}
+	\end{figure}
+	
+	
+	
+	\subsubsection*{Heuristic Stopping Criteria}
+	
+	Some studies give the example of heuristic stopping criteria based on drawing a given number of irrelevant articles in a row \cite{Jonnalagadda2013, Przybya2018}. 
+	We take this as a proxy for estimating that the proportion of documents remaining in the unseen documents is low, as the probability of observing 0 relevant documents in a given sample (analogous to a set of consecutive irrelevant results) is a decreasing function of the number of relevant documents in the population.
+	We find this a promising intuition, but argue that 1) it ignores uncertainty, as discussed in relation to the previous method; 
+	2) it lacks a formal description that would help to find a suitable threshold for the criterion; and 3) it misunderstands the significance of a low proportion of relevant documents in estimating the recall.
+	
+	Figure \ref{unseen-proportions} illustrates this third point. 
+	We show two scenarios with identical low proportions of relevant documents observed in the unseen documents.
+	In the top figure, machine learning (ML) has performed well, and 74\% of the screened documents were relevant. 
+	In the bottom figure, ML has performed less well, and only 26\% of the screened documents were relevant.
+	In both cases, only 2\% of unseen documents are relevant, but 2\% of a larger number means more relevant documents are missed.
+	Recall is not simply a function of the proportion of unseen relevant documents, but also of the number of unseen documents. 
+	This also means that where ML has performed well (as in the top figure), a low proportion of relevant documents in those not yet checked is indicative of lower recall than where ML has performed less well.
+	Likewise, where the proportion of relevant documents in the whole corpus is low, a similarly low proportion of relevant documents is likely to be observed, even when true recall is low. 
+	This shows us that even a perfect estimator of the proportion of unseen relevant documents is insufficient on its own to provide sufficient information about when to stop screening. To estimate recall reliably, it is necessary to take into account the total number of unseen relevant documents (or their proportion times the number of unseen documents).
+	
+	\subsubsection*{Pragmatic stopping criteria}
+	
+	Wallace et al. \cite{Wallace2010a} develop a ``simple, operational stopping criterion'': stopping after half the documents have been screened. Although the criterion worked in their experiment, it is unclear how this could be generalised, and its development depended on knowledge of the true relevance values. 
+	Jonnalagadda and Petitti \cite{Jonnalagadda2013} note that ``the reviewer can elect to end the process of classifying documents at any point, recognizing that stopping before reviewing all documents involves a trade-off of lower recall for reduced workload'', although clearly the reviewer lacks information about probable recall.
+	
+	\subsubsection*{Novel automatic stopping criteria}
+	Two examples come from the information retrieval literature. Di Nunzio \cite{DiNunzio2018} presents a novel automatic stopping criterion based on BM25, although recall reported is ``often between 0.92 and 0.94 and consistently over 0.7''.
+	Yu and Menzies \cite{Yu2019} also present a stopping criterion based on BM25 which allows the user to target a specific level of recall. 
+	However, reviewers are not given the opportunity to specify a confidence level, and for two of the four datasets in which they tested their criteria, the median achieved recall at a stopping criteria targeting 95\% recall was below 95\%. In each case, the reliability of the estimate is dependent on the performance of the model.
+	
+	Finally, Howard et al. \cite{Howard2020} present a method to estimate recall based on the number of irrelevant documents $D$ observed in a list of documents since the $\delta th$ previous relevant document. 
+	They reason that this should follow the negative binomial distribution based on the proportion of remaining relevant documents $p$, and use this information to estimate $\hat{p}$, and with this, the total number of relevant articles and the estimated recall.
+	
+	However, their method does not quantify uncertainty, but can only claim that the method ``\textit{tends} to result in a conservative estimate of recall'' (emphasis ours). This is not guaranteed by the criterion itself but rather a finding of the simulation with example datasets. Further the authors do not give sufficient information to reproduce their results, providing neither code (they describe their own proprietary software), nor an equation for $\hat{p}$. Additionally, the criterion requires a tuning parameter $\delta$, which users may have insufficient information to set optimally.
+	Lastly, because screening is a form of sampling without replacement, the negative hypergeometric distribution should be preferred to the negative binomial, even though the latter can be a good approximation for cases with large numbers of documents.
+	
+	These last examples are promising developments, but they all fail to take into account the needs of live systematic reviews, where the reliability of and ease of communication about recall are paramount, and the results are independent of model performance. In the following, we explain our own method, which provides clearly communicable estimates of recall, which manage uncertainty in a way robust to model performance.
+	
+	\subsection*{Methods}
+	
+	\subsection*{A Statistical Stopping Criterion for Active Learning}
+	
+	In our screening setup, we start off with $N_{tot}$ documents that are potentially relevant. $\rho_{tot}$ of these documents are actually relevant, but we don't know this value \textit{a priori}. As we screen relevant documents we include them, so $\rho_{seen}$ represents the number of relevant documents screened, and recall $\tau$ is given by 
+	
+	\begin{equation}
+	\tau = \frac{\rho_{seen}}{\rho_{tot}}
+	\label{eq:recall-def}
+	\end{equation}
+	
+	We set a target recall $\tau_{tar}$ and a confidence level $\alpha$. 
+	We want to keep screening until $\tau \geq \tau_{tar}$, and devise a hypothesis test to estimate whether this is the case with a given level of confidence. 
+	We do this	based on interrupting the active-learning process and drawing a random sample from the remaining unseen documents. 
+	We first describe this test, before showing how a variation on the test can be used to decide when to begin drawing a random sample. 
+	
 	\subsubsection*{Random Sampling}
 	
-	After using machine learning to select which documents are screened by humans as described above, we begin drawing a random sample from the remaining documents (when this happens is described below). 
-
-	We use random sampling to estimate the probability that a target recall $\tau$ has been achieved. Because we are sampling a binary outcome without replacement, we can use the hypergeometric distribution to formulate a statistical test. The hypergeometric distribution tells us the probability of observing $k$ relevant documents in $n$ draws from a finite population of $N$ documents with $K$ relevant documents. 
+	At the start of the sample, $N_{AL}$ is the number of documents seen during the active learning process, and $N$ is the number of documents remaining, so that 
+	
+	\begin{equation}
+	N = N_{tot} - N_{AL}
+	\end{equation} 
+	
+	We refer to the number  of relevant documents seen during active learning as $\rho_{AL}$, and the number of remaining relevant documents as $K$. We do not know the value of $K$ but know that it is given by the total number of relevant documents minus the number of relevant documents seen during active learning.
+	
+	\begin{equation}
+	K = \rho_{tot} - \rho_{AL}
+	\label{eq:K}
+	\end{equation}
+	
+	We now take random draws from the remaining $N$ documents, and denote the number of documents drawn with $n$ and the number of relevant documents drawn with $k$. The number of relevant documents seen is updated by adding the number of relevant documents seen since sampling began to the number of relevant documents seen during active learning.
 	
 	\begin{equation}
-		k \sim Hypergeometric(N, K, n)
+	\rho_{seen} = \rho_{AL} + k
+	\label{eq:rho_seen}
 	\end{equation}
 	
-	In our case, we know $k$, $n$ and $N$ after each draw, but $K$ is unknown. We therefore substitute a hypothetical value $\hat{K}$: the minimum number of relevant documents in the sample had the recall target been missed.
+	We proceed to form a null hypothesis that the true value of recall is less than our target recall:
 	
-	Recall $R$ is given by the number of relevant documents that have been seen $\rho_{s}$ over the number of relevant documents in the whole dataset $\rho_{tot}$
+	\begin{equation}
+	H_0 : \tau < \tau_{tar}
+	\label{eq:null_hypothesis}
+	\end{equation}
+	
+	Because we are sampling without replacement, we can use the hypergeometric distribution to find out the probability of observing $k$ relevant documents in a sample of $n$ documents from a population of $N$ documents of which $K$ are relevant. We know that $k$ is distributed hypergeometrically:
 	
 	\begin{equation}
-		R = \frac{\rho_{seen}}{\rho_{tot}}
+	k \sim Hypergeometric(N, K, n)
 	\end{equation}
 	
-	The number of relevant documents in the whole dataset is the sum of $\hat{\rho}$ relevant documents seen before random sampling began and $\tilde{K}$ relevant documents unseen at the start of random sampling. We can therefore express $R$ as
+	We introduce a hypothetical value for $K$, which we call $K_{tar}$. This represents the minimum number of relevant documents remaining at the start of sampling compatible with our null hypothesis that recall is below our target.
 	
 	\begin{equation}
-		R = \frac{\dot{\rho}}{\hat{\rho} + \tilde{K}},
+	K_{tar} = \lfloor \frac{\rho_{seen}}{\tau_{tar}}-\rho_{AL}+1 \rfloor
 	\end{equation}
 	
-	Substituting the target recall $\tau$ for $R$, reorganising and rounding up to the next integer, we can, after each draw, calculate $\hat{K}$, which is the minimum number of relevant documents that could have been remaining when random sampling started, if recall were lower than the target.
+	This equation is derived by combining Eqs.~\ref{eq:recall-def} and \ref{eq:rho_seen}. Because $k$ can only take integer values, $K_{tar}$ is the smallest integer that satisfies the inequality in Eq.~\ref{eq:null_hypothesis}.
+	With $K_{tar}$, we can reformulate our null hypothesis: the true number of relevant documents in the sample is greater than our hypothetical value.
 	
 	\begin{equation}
-		\hat{K} = \lceil \frac{\rho}{\tau} - \hat{\rho} \rceil
+	H_0 : K > K_{tar}
 	\end{equation}
 	
-	We use the cumulative distribution function of the hypergeometric distribution to estimate the probability $p$ of having observed $k$ or fewer relevant documents in the sample given $\hat{K}$. This function gives us an upper bound on the probability of observing no more than the number of relevant documents in our random sample that we did, if our recall target had not been achieved.
-	If this is below our confidence level $1 - \alpha$, we can reject the null hypothesis that the recall target was not achieved.
+	We test this by calculating the probability of observing $k$ or fewer relevant documents from the hypergeometric distribution given by $K_{tar}$, using the cumulative probability mass function.
 	
+	\begin{equation}
+	p = P(X \leq k)$, where $X \sim Hypergeometric(N,K_{tar},n)
+	\label{eq:p-value}
+	\end{equation}
 	
+	Because the cumulative probability mass function $P(X \leq k)$ is decreasing with increasing $K$, this gives the maximum probability of observing $k$ for all values of $K$ compatible with our null hypothesis. Similar arguments have been made to derive confidence intervals for estimating the parameter $K$ in the hypergeometric distribution function \citep{Buonaccorsi1987, Sahai1995} and the derivation of an equivalent criterion could use the upper limit of such a confidence interval of an estimated $K$ from the observation of $k$.
 	
-
-
-	\subsubsection*{Pseudo-random sampling}
+	We can reject our null hypothesis and stop screening if the maximum probability of obtaining our observed results given our null hypothesis $p$ is below $1-\alpha$. This means, we can report the likelihood that we achieve a recall above our target as being more than $\alpha$ \footnote{The notebook, \url{https://github.com/mcallaghan/rapid-screening/blob/master/analysis/hyper_criteria_theory.ipynb}, in the github repository accompanying this paper contains a step by step explanation of this method with code and examples}.
+	
+	
+	
+	\subsubsection*{Ranked quasi-sampling}
+	
+	We now proceed to describe a special case of the method described above which we (1) use as a heuristic in order to decide when to begin random sampling; and (2) test as an independent stopping criterion. The method works by treating batches of previously screened documents as if they were random samples.
+	
+	We calculate $p$ as above for subsets of the already screened documents. Concretely, we use subsets of documents $A_i$ by looking back to the last $i$ documents, $A_i = \{d_{N_{seen} - 1}, ..., d_{N_{seen} - i}\}$, where the documents $d$ are indexed in the order in which they have been screened. For a specific $i$, this corresponds to  random sampling beginning after seeing $i$ documents in the section above.
+	Thus, we set 
+	$N_{AL}$ to $i$, 
+	$n$ to $N_{seen}-i$, 
+	$\rho_{AL}$ to the number of relevant documents seen when $i$ documents had been seen,
+	and $k$ to the number of relevant documents seen since $i$ documents had been seen, and calculate $p$ according to Eq.~\ref{eq:p-value}.
+	We compute $p$ for all sets $A_i$ with $i \in {N_{seen}-1 \dots 1}$.
+	This gives us a vector $\bm{p}$, representing the values of $p$ which would have been estimated at each point at which we could have stopped active learning and began random sampling. The lowest probability of our null hypothesis being true that we would have thereby obtained is given by $p_{min}$. With the vectorized implementation included in our accompanying code, these calculations are completed in less than the time it would take a human to code the next document.
+	
+	First, we use this method as a useful heuristic for deciding when to stop active learning, and switch to random sampling. For this, we choose a higher threshold for the likelihood, $p_{min} < 1-\frac{\alpha}{2}$. Second, we use the same ranked quasi-sampling as an independent stopping criterion, by continuing screening with active learning until $p_{min} < 1 - \alpha$. We present the results of this second procedure separately below.
+	
+	Given that the documents seen during active learning are ranked according to predicted relevance, they do not in fact represent a random sample. This means that the test is unlikely to be accurate. It would be reasonable to assume that the proportion of relevant documents in each ranked quasi-sample is as high if not higher than the proportion of relevant documents in the unseen documents. This assumption would make this estimator conservative. As such it works in a similar way to the criterion proposed by Howard et al. \cite{Howard2020}, although it makes use of more information and provides hypothesis testing rather than just a point estimate of recall.
 	
-	In order to decide when to begin a random sample, we employ pseudo-random sampling, where we treat previously screened documents as a random sample. The distribution of relevant documents among previously screened documents is clearly not random, as documents predicted to be relevant are prioritised. It is reasonable to assume, though, that the density of relevant documents is greater among previously screened documents than among remaining unseen documents. This would make the following estimates conservative. 
 	
-	After reviewing each document, $S$ documents have been screened, and $U$ documents are yet to be seen. We treat $i = 1 \dots S$ of the previously screened documents as a random sample, and calculate $p$, using the method above, for each sample, taking the minimum across all samples $p_{min}$. If $p_{min}$ is less than $1-\frac{\alpha}{2}$, we switch to random sampling. We also calculate $p_{min}$ for the remaining documents as if we had not switched to random sampling and record the recall and work saved when $p_{min} < 1 - \alpha$. We present these in the results below as the psuedo-random sampling criterion.
-
-
 	\medskip
 	
 	\begin{figure}
-		\includegraphics[width=0.7\linewidth]{../images/flow}
+		\includegraphics[width=0.7\linewidth]{2_figs_flow.pdf}
 		\caption{A workflow for active learning in screening with a statistical stopping criterion}
 		\label{flow}
 	\end{figure}
-
+	
 	\subsection*{Evaluation}
 	
 	We evaluate each of the criteria discussed on real world test data, operationalising the heuristic stopping criteria with 50, 100, and 200 consecutive irrelevant records. We run 100 iterations on each dataset and record the following measures.
@@ -432,15 +493,15 @@ These last examples are promising developments, but fail to take into account th
 		\item \textbf{Actual Recall}: The recall when the stopping criteria was met
 		\item \textbf{WS-SC}: Work saved when the stopping criteria was met
 		\item \textbf{Additional Burden}: the work saved when the criterion was triggered subtracted from the work saved when the recall target was actually achieved.
-
+		
 	\end{itemize}
-	For simplicity, we use a basic SVM model \cite{Cortes95, Pedregosa2011}, with 1-2 word n-grams taken from the document abstracts used as input data. We start with random samples of 200 documents (we do not employ Shemilt et al's methods for identifying the ``optimal'' sample size, as we showed these in the methods section to be unhelpful). Subsequently, we ``screen'', that is, we reveal the labels of, batches of the 20 documents with the highest predicted relevance scores, retraining the model after each batch. Each criterion is evaluated after each document is ``screened''.
-	 For our criteria, we set the target recall value to 95\% and the confidence level to 95\%.
+	For simplicity, we use a basic SVM model \cite{Cortes95, Pedregosa2011}, with 1-2 word n-grams taken from the document abstracts used as input data. We start with random samples of 200 documents (we do not employ Shemilt et al's methods for identifying the ``optimal'' sample size, as we showed these in the methods section to be unhelpful). Subsequently, we ``screen'', that is, we reveal the labels of, batches of the 20 documents with the highest predicted relevance scores, retraining the model after each batch. Theoretically, using smaller batch sizes could mean that the recall target is achieved more quickly, but this is a trade-off between computational time spent training, and the speed at which the algorithm can ``learn''. However this is a modelling choice which may affect work saved, but not recall. Each criterion is evaluated after each document is ``screened''.
+	For our criteria, we set the target recall value to 95\% and the confidence level to 95\%.
 	
 	
 	%\subsubsection*{Evaluation Data}
 	\begin{table}
-		\input{../tables/datasets.tex}
+		\input{1_tables_datasets.tex}
 		\caption{Dataset properties}
 		\label{tab:data}
 	\end{table}
@@ -456,137 +517,141 @@ These last examples are promising developments, but fail to take into account th
 	Figure \ref{recall-wss} shows the actual recall and work savings achieved when each stopping criteria has been satisfied. 
 	For comparison, we also include the results that would have been achieved with \textit{a priori} knowledge of the data, that is, the work saved when the 95\% recall target was actually reached. In a live systematic review, reviewers would never know when this had been reached, but these are the work savings most often reported in machine learning for systematic review screening studies.
 	
-
-    \begin{figure*}
-	\centering
-	\begin{subfigure}[b]{0.475\textwidth}   
-		\centering 
-		\includegraphics[width=\textwidth]{../images/jointplot_hyper}
-		\caption[]%
-		{{\small Hypergeometric sampling \\}}    
-		\label{fig:hyper}
-	\end{subfigure}
-	\hfill
-	\begin{subfigure}[b]{0.475\textwidth}   
-		\centering 
-		\includegraphics[width=\textwidth]{../images/jointplot_nrs}
-		\caption[]%
-		{{\footnotesize Pseudorandom hypergeometric sampling}}    
-		\label{fig:nrs}
-	\end{subfigure}
-	\vskip\baselineskip
-	\begin{subfigure}[b]{0.475\textwidth}
+	
+	\begin{figure*}
 		\centering
-		\includegraphics[width=\textwidth]{../images/jointplot_bir}
-		\caption[Network2]%Example-Image
-		{{\small Baseline Sampling}}    
-		\label{fig:bir}
-	\end{subfigure}
-	\hfill
-	\begin{subfigure}[b]{0.475\textwidth}  
-		\centering 
-		\includegraphics[width=\textwidth]{../images/jointplot_ih_50}
-		\caption[]%
-		{{\small 50 consecutive irrelevant results}}    
-		\label{fig:ih_50}
-	\end{subfigure}
-	\vskip\baselineskip
-	\begin{subfigure}[b]{0.475\textwidth}   
-		\centering 
-		\includegraphics[width=\textwidth]{../images/jointplot_ih_200}
-		\caption[]%
-		{{\small 200 consecutive irrelevant results \\}}    
-		\label{fig:ih_200}
-	\end{subfigure}
-	\hfill
-	\begin{subfigure}[b]{0.475\textwidth}   
-		\centering 
-		\includegraphics[width=\textwidth]{../images/jointplot_pf.pdf}
-		\caption[]%
-		{{\footnotesize \textit{a priori} knowledge}}   
-		\label{fig:pf}
-	\end{subfigure}
-
-	\caption{\small Distribution of recall and work saved after each stopping criteria. Green dots show results for datasets with less than 1,000 documents, orange dots show datasets with 1,000 - 2,000 documents, and blue dots show datasets with more than 2,000 documents.} 
-	\label{recall-wss}
-\end{figure*}
-
-\begin{figure*}
-	\centering
 		\begin{subfigure}[b]{0.475\textwidth}   
-		\centering 
-		\includegraphics[width=\textwidth]{../images/jointplot_burden_hyper}
-		\caption[]%
-		{{\small Hypergeometric sampling \\}}    
-		\label{fig:hyper_ab}
-	\end{subfigure}
-	\hfill
-	\begin{subfigure}[b]{0.475\textwidth}   
-		\centering 
-		\includegraphics[width=\textwidth]{../images/jointplot_burden_nrs.pdf}
-		\caption[]%
-		{{\footnotesize Pseudorandom hypergeometric sampling}}    
-		\label{fig:nrs_ab}
-	\end{subfigure}
-	\hfill
-	\vskip\baselineskip
-	\begin{subfigure}[b]{0.475\textwidth}
+			\centering 
+			\includegraphics[width=\textwidth]{2_figs_jointplot_hyper.pdf}
+			\caption[]%
+			{{\small Hypergeometric sampling \\}}    
+			\label{fig:hyper}
+		\end{subfigure}
+		\hfill
+		\begin{subfigure}[b]{0.475\textwidth}   
+			\centering 
+			\includegraphics[width=\textwidth]{2_figs_jointplot_nrs.pdf}
+			\caption[]%
+			{{\footnotesize Ranked quasi-sampling}}    
+			\label{fig:nrs}
+		\end{subfigure}
+		\vskip\baselineskip
+		\begin{subfigure}[b]{0.475\textwidth}
+			\centering
+			\includegraphics[width=\textwidth]{2_figs_jointplot_bir.pdf}
+			\caption[Network2]%Example-Image
+			{{\small Baseline Sampling}}    
+			\label{fig:bir}
+		\end{subfigure}
+		\hfill
+		\begin{subfigure}[b]{0.475\textwidth}  
+			\centering 
+			\includegraphics[width=\textwidth]{2_figs_jointplot_ih_50.pdf}
+			\caption[]%
+			{{\small 50 consecutive irrelevant results}}    
+			\label{fig:ih_50}
+		\end{subfigure}
+		\vskip\baselineskip
+		\begin{subfigure}[b]{0.475\textwidth}   
+			\centering 
+			\includegraphics[width=\textwidth]{2_figs_jointplot_ih_200.pdf}
+			\caption[]%
+			{{\small 200 consecutive irrelevant results \\}}    
+			\label{fig:ih_200}
+		\end{subfigure}
+		\hfill
+		\begin{subfigure}[b]{0.475\textwidth}   
+			\centering 
+			\includegraphics[width=\textwidth]{2_figs_jointplot_pf.pdf}
+			\caption[]%
+			{{\footnotesize \textit{a priori} knowledge}}   
+			\label{fig:pf}
+		\end{subfigure}
+		
+		\caption{\small Distribution of recall and work saved after each stopping criteria. Green dots show results for datasets with less than 1,000 documents, orange dots show datasets with 1,000 - 2,000 documents, and blue dots show datasets with more than 2,000 documents.} 
+		\label{recall-wss}
+	\end{figure*}
+	
+	\begin{figure*}
 		\centering
-		\includegraphics[width=\textwidth]{../images/jointplot_burden_bir}
-		\caption[Network2]%Example-Image
-		{{\small Baseline Sampling}}    
-		\label{fig:bir_ab}
-	\end{subfigure}
-	\hfill
-	\begin{subfigure}[b]{0.475\textwidth}  
-		\centering 
-		\includegraphics[width=\textwidth]{../images/jointplot_burden_ih_50}
-		\caption[]%
-		{{\small 50 consecutive irrelevant results}}    
-		\label{fig:ih_50_ab}
-	\end{subfigure}
-	\vskip\baselineskip
-
-	\begin{subfigure}[b]{0.475\textwidth}   
-		\centering 
-		\includegraphics[width=\textwidth]{../images/jointplot_burden_ih_100}
-		\caption[]%
-		{{\small 100 consecutive irrelevant results }}    
-		\label{fig:ih_200_ab}		
-
-	\end{subfigure}
-	\hfill
-	\begin{subfigure}[b]{0.475\textwidth}   
-		\centering 
-		\includegraphics[width=\textwidth]{../images/jointplot_burden_ih_200.pdf}
-		\caption[]%
-		{{\footnotesize 200 consecutive irrelevant results }}    
-		\label{fig:ih_100_ab}
-	\end{subfigure}
-	
-	\caption{\small Distribution of recall and additional burden after each stopping criterion. Additional burden is the work saved when the criterion was triggered minus the work saved when the target was reached. Coloring of data points as in Fig. \ref{recall-wss}.}
-	\label{recall-burden}
-\end{figure*}
-
-
+		\begin{subfigure}[b]{0.475\textwidth}   
+			\centering 
+			\includegraphics[width=\textwidth]{2_figs_jointplot_burden_hyper.pdf}
+			\caption[]%
+			{{\small Hypergeometric sampling \\}}    
+			\label{fig:hyper_ab}
+		\end{subfigure}
+		\hfill
+		\begin{subfigure}[b]{0.475\textwidth}   
+			\centering 
+			\includegraphics[width=\textwidth]{2_figs_jointplot_burden_nrs.pdf}
+			\caption[]%
+			{{\footnotesize Ranked quasi-sampling}}    
+			\label{fig:nrs_ab}
+		\end{subfigure}
+		\hfill
+		\vskip\baselineskip
+		\begin{subfigure}[b]{0.475\textwidth}
+			\centering
+			\includegraphics[width=\textwidth]{2_figs_jointplot_burden_bir.pdf}
+			\caption[Network2]%Example-Image
+			{{\small Baseline Sampling}}    
+			\label{fig:bir_ab}
+		\end{subfigure}
+		\hfill
+		\begin{subfigure}[b]{0.475\textwidth}  
+			\centering 
+			\includegraphics[width=\textwidth]{2_figs_jointplot_burden_ih_50.pdf}
+			\caption[]%
+			{{\small 50 consecutive irrelevant results}}    
+			\label{fig:ih_50_ab}
+		\end{subfigure}
+		\vskip\baselineskip
+		
+		\begin{subfigure}[b]{0.475\textwidth}   
+			\centering 
+			\includegraphics[width=\textwidth]{2_figs_jointplot_burden_ih_100.pdf}
+			\caption[]%
+			{{\small 100 consecutive irrelevant results }}    
+			\label{fig:ih_200_ab}		
+			
+		\end{subfigure}
+		\hfill
+		\begin{subfigure}[b]{0.475\textwidth}   
+			\centering 
+			\includegraphics[width=\textwidth]{2_figs_jointplot_burden_ih_200.pdf}
+			\caption[]%
+			{{\footnotesize 200 consecutive irrelevant results }}    
+			\label{fig:ih_100_ab}
+		\end{subfigure}
+		
+		\caption{\small Distribution of recall and additional burden after each stopping criterion. Additional burden is the work saved when the criterion was triggered minus the work saved when the target was reached. Coloring of data points as in Fig. \ref{recall-wss}.}
+		\label{recall-burden}
+	\end{figure*}
+	
+	
+	
+	Both the random sampling and the ranked sampling criteria achieve the target threshold of 95\% in more than 95\% of cases. 
+	That this is greater than 95\% is accounted for by the fact that random sampling sometimes begins after the target has been achieved, in which case the null hypothesis would be \text{a priori} impossible.
+	The ranked quasi-sampling criterion  outperforms the random sampling criterion with respect to both recall and work savings, saving a mean of 17\% of the work compared to 15\%, and missing the target in only 0.95\% compared to 3.29\% of cases. In theory, the ranked sampling criteria is conservative if the assumption holds that documents chosen by machine learning are not less likely to be relevant than those chosen at random. Based on our experiments, this assumption seems reasonable, and accounts for the higher recall. Because the ranked quasi-sampling criterion can flexibly choose its sample, whereas the random criterion has to wait for a random sample to be triggered, the criterion is also triggered earlier, as it can make use of more data. This accounts for the higher work savings.
+	
 	
-	Both the random sampling and the pseudorandom sampling criteria achieve the target threshold of 95\% in more than 95\% of cases. In fact, the pseudorandom sampling criterion  outperforms the random sampling criterion with respect to both recall and work savings, saving a mean of 17\% of the work compared to 15\%, and missing the target in only 0.95\% compared to 3.29\% of cases. In theory, the pseudorandom sampling criteria is conservative if the assumption holds that documents chosen by machine learning are not less likely to be relevant than those chosen at random. Based on our experiments, this assumption seems reasonable, and accounts for the higher recall. Because the pseudorandom sampling criterion can flexibly choose its sample, whereas the random criterion has to wait for a random sample to be triggered, the criterion is also triggered earlier, as it can make use of more data. This accounts for the higher work savings.
 	
 	The baseline sampling criteria (Figure \ref{fig:bir}) misses the 95\% recall target in 39.67\% of cases, while the most common work saving is 0\%. This is in line with our expectations that, due to random sampling error, the expected number of documents will often be over-estimated or under-estimated, resulting in zero work savings or poor recall.
 	
 	The Heuristic stopping criteria, both for 50 consecutive irrelevant results (Figure \ref{fig:ih_50} - IH50), and for 200 irrelevant results (Figure \ref{fig:ih_200}) also perform unreliably. Although the mean work saved for IH50 is 41\%, the target is missed in 39\% of cases. The cases below the horizontal grey line indicate instances where work has been saved at the expense of achieving the recall target.
 	
 	\begin{figure}
-		\includegraphics[width=0.9\linewidth]{../images/wss_nrs}
-		\caption{Work saved for the pseudorandom sampling method in each dataset. Labels show the number of relevant documents and the total number of documents. The datasets are presented in order of the number of documents. The whiskers represent the 5th and 95th percentiles. The grey line shows work savings of 5\%. }
+		\includegraphics[width=0.9\linewidth]{2_figs_wss_nrs.pdf}
+		\caption{Work saved for the ranked quasi-sampling method in each dataset. Labels show the number of relevant documents and the total number of documents. The datasets are presented in order of the number of documents. The whiskers represent the 5th and 95th percentiles. The grey line shows work savings of 5\%. }
 		\label{wss}
 	\end{figure}
 	
- 	\begin{figure*}
+	\begin{figure*}
 		\centering
 		\begin{subfigure}[b]{0.475\textwidth}
 			\centering
-			\includegraphics[width=\textwidth]{../images/h0_paths_Radjenovic.pdf}
+			\includegraphics[width=\textwidth]{2_figs_h0_paths_Radjenovic.pdf}
 			\caption[Network2]%Example-Image
 			{{\small Radjenovic}}    
 			\label{fig:Radjenovic}
@@ -594,15 +659,15 @@ These last examples are promising developments, but fail to take into account th
 		\hfill
 		\begin{subfigure}[b]{0.475\textwidth}  
 			\centering 
-			\includegraphics[width=\textwidth]{../images/h0_paths_ProtonBeam.pdf}
+			\includegraphics[width=\textwidth]{2_figs_h0_paths_ProtonBeam.pdf}
 			\caption[]%
 			{{\small ProtonBeam}}    
 			\label{fig:ProtonBeam}
 		\end{subfigure}
-	\vskip\baselineskip
+		\vskip\baselineskip
 		\begin{subfigure}[b]{0.475\textwidth}
 			\centering
-			\includegraphics[width=\textwidth]{../images/h0_paths_Statins.pdf}
+			\includegraphics[width=\textwidth]{2_figs_h0_paths_Statins.pdf}
 			\caption[Network2]%Example-Image
 			{{\small Statins}}    
 			\label{fig:Statins}
@@ -610,19 +675,19 @@ These last examples are promising developments, but fail to take into account th
 		\hfill
 		\begin{subfigure}[b]{0.475\textwidth}  
 			\centering 
-			\includegraphics[width=\textwidth]{../images/h0_paths_Triptans.pdf}
+			\includegraphics[width=\textwidth]{2_figs_h0_paths_Triptans.pdf}
 			\caption[]%
 			{{\small Triptans}}    
 			\label{fig:Triptans}
 		\end{subfigure}
-	
+		
 		\caption{\small The path of recall (yellow) and the p-value of H0 for four different datasets} 
 		\label{H0paths}
 	\end{figure*}
 	
 	In figure \ref{recall-burden} we rescale the x axis, calling it additional burden, which is simply the work saved when the criterion is triggered minus the work saved when the recall target was actually achieved. This measure indicates whether the stopping criterion was triggered too early (negative values), or too late (positive values). The figure directly highlights the tradeoffs involved in deciding when to stop screening: For our criteria, there is mostly a small additional burden which comes with the necessity to make sure the desired recall target has been reached and reject the null hypothesis that this has not been the case. For the other criteria, there are many cases in which additional burden is negative, i.e. the criterion has been triggered too early. In these cases, however, the desired recall is hardly ever reached.
 	
-	To help explain the different work savings that were observed in our experiments, we show the distribution of work savings from our pseudorandom criterion for each dataset in figure \ref{wss}. In general, higher work savings are possible when the total number of documents is larger. However, in datasets with a low proportion of relevant documents, many documents need to be screened to achieve a high confidence that there are only few relevant documents remaining in the unseen ones. Therefore, smaller work savings are possible. 
+	To help explain the different work savings that were observed in our experiments, we show the distribution of work savings from our ranked quasi-sampling criterion for each dataset in figure \ref{wss}. In general, higher work savings are possible when the total number of documents is larger. However, in datasets with a low proportion of relevant documents, many documents need to be screened to achieve a high confidence that there are only few relevant documents remaining in the unseen ones. Therefore, smaller work savings are possible. 
 	
 	Figure \ref{H0paths} shows the recall and the probability of the null hypothesis for the best performing iteration of four datasets. Although the 95\% recall target is achieved very quickly in the Radjenovic dataset, the null hypothesis cannot be excluded until much later. This is because the dataset has only 47 relevant documents out of a population of 5,999. After the 95\% recall target was achieved, 45 out of 47 relevant documents had been seen and 5,029 documents remained. The null hypothesis was therefore that 3 or more of these 5,029 documents were relevant, which requires a lot of evidence to disprove. The burden of proof was smaller in the case of the Proton Beam dataset: at the point that the 95\% recall threshold was reached, the null hypothesis to disprove was that a minimum of 13 out of 3,369 remaining documents were relevant. 
 	
@@ -632,32 +697,43 @@ These last examples are promising developments, but fail to take into account th
 	
 	Our results show that it is possible to use machine learning to achieve a given level of recall with a given level of confidence. The tradeoff for achieving recall reliably is that the work saving achieved is less than the maximum possible work saving. However, for large datasets with a significant proportion of relevant documents, the additional effort required to satisfy the criterion will be small compared to the work saved by using machine learning. This makes the approach well suited to broad topics with lots of literature. In other words, it is precisely where machine learning will be most useful that the additional effort will be small.
 	
-	Different use cases for machine learning enhanced screening may also carry different requirements for recall, or different tolerances for uncertainty. These can be flexibly accommodated within our stopping criterion. Importantly, the ability to make probabilistic statements about the chance of achieving a given recall target makes it possible to clearly communicate the implications of using machine learning enhanced screening to readers and reviewers who are not machine learning specialists. This is extremely important in live systematic reviews. 
+	Different use cases for machine learning enhanced screening may also carry different requirements for recall, or different tolerances for uncertainty. These can be flexibly accommodated within our stopping criterion. Importantly, the ability to make probabilistic statements about the chance of achieving a given recall target makes it possible to clearly communicate the implications of using machine learning enhanced screening to readers and reviewers who are not machine learning specialists. This is extremely important in live systematic reviews.
 	
 	Our criteria have the further advantage that they are independent of the choice or performance of the machine learning model. If a model performs badly at discerning relevant from irrelevant results, the only consequence will be that the work saved will be low. With other criteria this may result in poor recall. 
 	When using machine learning for screening, poor recall can result in biased results, while low work savings represent no loss to the reviewer as compared to not using machine learning.
 	
+	One caveat in the derivation of our criteria is that we did not address the potential problem of multiple testing formally. Such a derivation is mathematically challenging and beyond the scope of this paper. However, the performance of the criteria shows that this is of limited practical concern. Formally describing screening procedures with iterative testing should be a next step towards even more rigorous stopping criteria and should be fully worked out in future research.
+	
 	So far, systematic review standards have no way of accommodating screening with machine learning. 
 	We hope that the reliability and clarity of reporting offered by our stopping criteria make them suitable for incorporation into standards, so that machine learning for systematic review screening can fulfil its promise of reducing workload and making more ambitious reviews tractable.
 	
 	\section*{Conclusion}
 	
-	This paper demonstrates the unsuitability of existing stopping criteria for machine learning approaches to document screening, and proposes a simple method that delivers reliable recall, independent of machine learning approach or model performance. Our robust statistical stopping criteria allow users to easily communicate the implications of their use of machine learning, making machine learning enhanced screening ready for live reviews.
+	This paper demonstrates the drawbacks of existing stopping criteria for machine learning approaches to document screening, particularly with regard to reliability. We propose a simple method that delivers reliable recall, independent of machine learning approach or model performance. Our robust statistical stopping criteria allow users to easily communicate the implications of their use of machine learning, making machine learning enhanced screening ready for live reviews.
 	
 	
-\begin{backmatter}
-	
-	\section*{Competing interests}
-	The authors declare that they have no competing interests.
-	
-	\section*{Author's contributions}
-	MC designed the research and conducted the experiments. FMH contributed to the development of the statistical basis for the stopping criterion. Both authors wrote and edited the manuscript.
-	
-	\section*{Acknowledgements}
-	Max Callaghan is supported by a PhD scholarship from the Heinrich Böll Foundation. Finn M\"{u}ller-Hansen acknowledges funding from the German Federal Ministry of Research and Education within the Strategic Scenario Analysis (START) project (grant reference: 03EK3046B).
+	\begin{backmatter}
 		
-	\bibliography{mendeley}
-	\bibliographystyle{vancouver}
-	
-\end{backmatter}
+		\section*{Ethics approval}
+		Not applicable.
+		
+		\section*{Consent for publication}
+		Not applicable.
+		
+		\section*{Availability of data and materials} 
+		All computational steps required to reproduce this analysis are documented online at \url{https://github.com/mcallaghan/rapid-screening}.
+		
+		\section*{Competing interests}
+		The authors declare that they have no competing interests.
+		
+		\section*{Author's contributions}
+		MC designed the research and conducted the experiments. FMH contributed to the development of the statistical basis for the stopping criterion. Both authors wrote and edited the manuscript.
+		
+		\section*{Acknowledgements}
+		Max Callaghan is supported by a PhD scholarship from the Heinrich Böll Foundation. Finn M\"{u}ller-Hansen acknowledges funding from the German Federal Ministry of Research and Education within the Strategic Scenario Analysis (START) project (grant reference: 03EK3046B).
+		
+		\bibliography{3_bib_mendeley}
+		\bibliographystyle{vancouver}
+		
+	\end{backmatter}
 \end{document}
\ No newline at end of file
